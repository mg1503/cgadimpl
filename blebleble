// =====================
// file: cgadimpl/src/graph.cpp
// =====================
#include "ad/graph.hpp"
#include <unordered_set>
#include <unordered_map>
#include <functional>
#include <cassert>
#include <iostream> 
#include <variant>
#include <sstream> // Required for std::stringstream for MLIR emission
#include <stdexcept> // Required for std::runtime_error
#include <type_traits> // Required for std::decay_t and std::is_same_v
#include "ad/runtime.hpp"

namespace ag {

// --- Node Implementation ---
Node::Node(const Tensor& v, Op op_, bool req_grad, const char* nm) 
    : op(op_), 
      value(v),
      requires_grad_flag_(req_grad),
      debug_name(nm) 
{
    if (requires_grad_flag_) {
        // Calls the 'zeros' factory with the correct signature (shape, opts).
        grad = OwnTensor::Tensor::zeros(v.shape(), ag::options(v));
    }
}

// --- Value Implementation ---
Tensor& Value::val() { return node->value; }
const Tensor& Value::val() const { return node->value; }
Tensor& Value::grad() { return node->grad; }
const Tensor& Value::grad() const { return node->grad; }
Value::Value() = default;
Value::Value(std::shared_ptr<Node> n) : node(std::move(n)) {}

// NEW: Implementation for the real shape()
const std::vector<int64_t>& Value::shape() const {
    return node->value.shape().dims;
}
// 2d helper
std::pair<int, int> Value::shape_2d() const {
    const auto& dims = node->value.shape().dims;
    if (dims.size() == 0) return {0, 0};
    if (dims.size() == 1) return {1, static_cast<int>(dims[0])};
    // For 2D or more, return the first two dimensions.
    return {static_cast<int>(dims[0]), static_cast<int>(dims[1])};
}

// --- Internal implementation for graph traversal ---
static std::vector<Node*> build_topo_order_impl(Node* root) {
    std::vector<Node*> order; order.reserve(256);
    std::unordered_set<Node*> vis; vis.reserve(256);
    std::function<void(Node*)> dfs = [&](Node* n){ if(!n || vis.count(n)) return; vis.insert(n); for(auto& p : n->inputs) dfs(p.get()); order.push_back(n); };
    dfs(root);
    return order; // parents before child
}

// A cache for memoizing topological sorts of graphs.
static std::unordered_map<Node*, std::vector<Node*>> topo_cache;

// --- Graph Traversal ---
std::vector<Node*> topo_from(Node* root){
    auto it = topo_cache.find(root);
    if (it != topo_cache.end()) {
        return it->second; // Cache hit
    }
    std::cout << "--- Building and Caching Computational Graph ---" << std::endl;
    std::vector<Node*> order = build_topo_order_impl(root);
    topo_cache[root] = order;
    return order;
}

} // namespace ag

// ===================================================================
// JIT COMPILER IMPLEMENTATION
// ===================================================================  

namespace ag::jit {

// Forward declare Compiled::Impl to use it in emitMLIR
struct Compiled::Impl;

struct TensorMetadata {
    std::vector<int64_t> shape;
    Dtype dtype;
    DeviceIndex device;
};

struct Signature {
    std::vector<TensorMetadata> in_meta;
    std::vector<TensorMetadata> param_meta;

    bool matches(const std::vector<Tensor*>& inputs,
                 const std::vector<Tensor*>& params) const {
        if (inputs.size() != in_meta.size() || params.size() != param_meta.size()) {
            return false;
        }

        for (size_t i = 0; i < inputs.size(); ++i) {
            if (inputs[i]->shape().dims != in_meta[i].shape ||
                inputs[i]->dtype() != in_meta[i].dtype ||
                inputs[i]->device().device != in_meta[i].device.device ||
                inputs[i]->device().index != in_meta[i].device.index) {
                return false;
            }
        }
        for (size_t i = 0; i < params.size(); ++i) {
            if (params[i]->shape().dims != param_meta[i].shape ||
                params[i]->dtype() != param_meta[i].dtype ||
                params[i]->device().device != param_meta[i].device.device ||
                params[i]->device().index != param_meta[i].device.index) {
                return false;
            }
        }
        return true;
    }
};
// Arg sources for a Step
struct ArgInput  { int idx; };   // external input[i]
struct ArgParam  { int idx; };   // external param[i]
struct ArgSlot   { int slot; };  // prior computed slot
struct ArgLit    { Tensor t{OwnTensor::Shape{}, OwnTensor::Dtype::Float32}; };  // embedded literal

using Arg = std::variant<ArgInput,ArgParam,ArgSlot,ArgLit>;

struct Step {
    Op op;
    std::vector<Arg> args;
    int out_slot{};
    TensorMetadata out_meta;
};

struct Plan {
    Signature sig;
    std::vector<Step> steps;
    int num_slots{0};
    int out_slot{-1};
};


struct Compiled::Impl {
    Plan plan;
    // The run method now uses the plan stored here.

    // --- helpers for replay ---
    static const Tensor& as_ref(const Arg& a,
                                const std::vector<Tensor*>& inputs,
                                const std::vector<Tensor*>& params,
                                const std::vector<Tensor>& slots,
                                Tensor& tmp) {
        if (std::holds_alternative<ArgInput>(a))  return *inputs[std::get<ArgInput>(a).idx];
        if (std::holds_alternative<ArgParam>(a))  return *params[std::get<ArgParam>(a).idx];
        if (std::holds_alternative<ArgSlot>(a))   return slots[std::get<ArgSlot>(a).slot];
        // literal: copy into tmp to return a ref
        const Tensor& lit = std::get<ArgLit>(a).t;
        tmp = lit;
        return tmp;
    }


    static Tensor apply(Op op, const std::vector<const Tensor*>& a) {
        switch(op){
            case Op::Add:        return *a[0] + *a[1];
            case Op::Sub:        return *a[0] - *a[1];
            case Op::Mul:        return *a[0] * *a[1];
            case Op::Div:        return *a[0] / *a[1];
            // case Op::Pow:        return OwnTensor::pow(*a[0], *a[1]);
            // case Op::Neg:        return -*a[0];

            // Unary operators
            // case Op::Transpose:  return a[0]->transpose(-2, -1);
            // case Op::ReLU:       { cudaStream_t stream = (cudaStream_t)ag::current_stream(); return (*a[0] + OwnTensor::abs(*a[0], stream)) * 0.5f;}
            case Op::Exp:        return OwnTensor::exp(*a[0]);
            case Op::Log:        return OwnTensor::log(*a[0]);
            case Op::Tanh:       return OwnTensor::tanh(*a[0]);
            // Missing: Sigmoid, Softplus, SiLU, GELU, LeakyRelu, Abs

            case Op::MatMul:     return OwnTensor::matmul(*a[0], *a[1]);

            // Reductions
            case Op::Sum: return OwnTensor::reduce_sum(*a[0]);
            case Op::RowSum: return OwnTensor::reduce_sum(*a[0], {1}, true);
            case Op::RowMax: return OwnTensor::reduce_max(*a[0], {1}, true);
            case Op::MeanAll: return OwnTensor::reduce_mean(*a[0]);

            // Missing: SoftmaxRow, LogSumExpRow, CeWithLogits, Reshape

            case Op::Leaf: default: {
                assert(false && "apply(): unexpected op");
                return *a[0];
            }
        }
    }


    bool run(const std::vector<Tensor*>& inputs,
             const std::vector<Tensor*>& params,
             Tensor& out) const {
        if (!plan.sig.matches(inputs, params)) return false;

        std::vector<Tensor> slots(plan.num_slots);
        
        // Execute
        Tensor tmp_as_ref{OwnTensor::Shape{}, OwnTensor::TensorOptions{}}; 

        for (const Step& st : plan.steps) {
            std::vector<const Tensor*> args; args.reserve(st.args.size());
            
            std::vector<Tensor> tmp_keep; tmp_keep.reserve(st.args.size());
            for (const Arg& a : st.args) {
                if (std::holds_alternative<ArgLit>(a)) {
                    tmp_keep.emplace_back(std::get<ArgLit>(a).t);
                    args.push_back(&tmp_keep.back());
                } else {
                    args.push_back(&as_ref(a, inputs, params, slots, tmp_as_ref));
                }
            }
            Tensor y = apply(st.op, args);
            slots[st.out_slot] = std::move(y);
        }

        out = slots[plan.out_slot];
        return true;
    }
};

static bool is_in(const std::unordered_map<Node*,int>& m, Node* n){ return m.find(n)!=m.end(); }


// -------------------------------------------------------------------
// MLIR Generation Logic (New)
// -------------------------------------------------------------------

// Helper to convert Dtype to MLIR tensor type string
static std::string dtypeToMLIR(Dtype dt) {
    switch (dt) {
        case OwnTensor::Dtype::Float32:  return "f32";
        case OwnTensor::Dtype::Float16:  return "f16";
        case OwnTensor::Dtype::Bfloat16: return "bf16";
        case OwnTensor::Dtype::Int32:    return "i32";
        case OwnTensor::Dtype::Int64:    return "i64";
        default:              return "unknown";
    }
}

// Helper to format shape as string
static std::string shapeToMLIR(const std::vector<int64_t>& shape) {
    std::string s;
    for (int64_t dim : shape) {
        s += std::to_string(dim) + "x";
    }
    if (!s.empty()) s.pop_back();
    return s;
}

// Helper function to map your Op enum to a specific 'nova' dialect operation name
// This mapping is crucial to match your desired output format
static std::string opToNovaOp(Op op) {
    switch (op) {
        case Op::Add:       return "nova.add";
        case Op::Mul:       return "nova.mul";
        case Op::MatMul:    return "nova.matmul"; 
        // Add other mappings as needed
        default:            return "nova.unknown_op";
    }
}


static std::string emitMLIR(const Plan& plan) {
    std::stringstream ss;
    
    // --- Function Signature ---
    ss << "func.func @main(";
    
    // Define all external inputs/params as %arg0, %arg1, ...
    size_t arg_idx_counter = 0;

    auto print_arg_meta = [&](const std::vector<TensorMetadata>& metas) {
        for (size_t i = 0; i < metas.size(); ++i) {
            const auto& meta = metas[i];
            ss << "%arg" << arg_idx_counter++ << ": tensor<" 
               << shapeToMLIR(meta.shape) << dtypeToMLIR(meta.dtype) << ">";
            if (i < metas.size() - 1 || !plan.sig.param_meta.empty()) ss << ", ";
        }
    };

    print_arg_meta(plan.sig.in_meta);
    // print_arg_meta(plan.sig.param_meta); // Enable if params should also be func args

    ss << ") -> tensor<" 
       << shapeToMLIR(plan.steps.back().out_meta.shape) 
       << dtypeToMLIR(plan.steps.back().out_meta.dtype) << "> {\n";


    // --- Function Body (Steps) ---
    std::unordered_map<int, std::string> slot_to_var_name; // Maps plan slot IDs to MLIR variable names (%v0, %v1, %a, %b)
    std::unordered_map<int, TensorMetadata> slot_to_meta; // Maps plan slot IDs to their metadata for type lookup

    // Store metadata for slots created so far (needed for backward lookup)
    for (const auto& st : plan.steps) {
        slot_to_meta[st.out_slot] = st.out_meta;
    }

    for (size_t i = 0; i < plan.steps.size(); ++i) {
        const auto& st = plan.steps[i];
        std::string result_var = "%v" + std::to_string(i); // Using %v0, %v1 for now
        slot_to_var_name[st.out_slot] = result_var;

        ss << "  " << result_var << " = " << opToNovaOp(st.op) << " ";

        std::vector<std::string> arg_names;
        std::vector<std::string> arg_types;

        // Process arguments
        for (const auto& arg : st.args) {
            std::visit([&](auto&& a) {
                using T = std::decay_t<decltype(a)>;
                if constexpr (std::is_same_v<T, ArgInput> || std::is_same_v<T, ArgParam>) {
                    // This assumes inputs and params are mapped correctly to %arg0, %arg1, ...
                    // If you have params, the index calculation needs adjustment.
                    int arg_idx = a.idx; 
                    // Simplified lookup for demo: assumes all inputs/params indexed linearly from 0 in sig
                    const auto& meta = (std::is_same_v<T, ArgInput>) ? plan.sig.in_meta[arg_idx] : plan.sig.param_meta[arg_idx];
                    arg_names.push_back("%arg" + std::to_string(arg_idx));
                    arg_types.push_back("tensor<" + shapeToMLIR(meta.shape) + dtypeToMLIR(meta.dtype) + ">");
                } else if constexpr (std::is_same_v<T, ArgSlot>) {
                    arg_names.push_back(slot_to_var_name.at(a.slot));
                    // Look up the type from the stored slot metadata
                    const auto& meta = slot_to_meta.at(a.slot);
                    arg_types.push_back("tensor<" + shapeToMLIR(meta.shape) + dtypeToMLIR(meta.dtype) + ">");
                } else if constexpr (std::is_same_v<T, ArgLit>) {
                    throw std::runtime_error("Literal handling in JIT emitMLIR not implemented for standard format.");
                }
            }, arg);
        }

        // Print arguments and types in the required format
        for (size_t j = 0; j < arg_names.size(); ++j) {
            ss << arg_names[j];
            if (j < arg_names.size() - 1) ss << ", ";
        }
        
        ss << ": ";

         for (size_t j = 0; j < arg_types.size(); ++j) {
            ss << arg_types[j];
            if (j < arg_types.size() - 1) ss << ", ";
        }
        
        ss << "\n";
    }

    // --- Return Statement ---
    std::string return_var = slot_to_var_name.at(plan.out_slot);
    // Get return type meta from the final step's output meta
    const auto& return_meta = plan.steps.back().out_meta;

    ss << "  return " << return_var << " : tensor<"
       << shapeToMLIR(return_meta.shape) 
       << dtypeToMLIR(return_meta.dtype) << ">\n";
       
    ss << "}\n";

    return ss.str();
}

// -------------------------------------------------------------------
// Compile function
// -------------------------------------------------------------------

Compiled compile(const Value& output,
                 const std::vector<Value>& inputs,
                 const std::vector<Value>& params,
                 const CompileOptions&) {
    std::unordered_map<Node*,int> in_ix, par_ix;
    for (size_t i = 0; i < inputs.size(); ++i) in_ix[inputs[i].node.get()] = i;
    for (size_t i = 0; i < params.size(); ++i) par_ix[params[i].node.get()] = i;

    Plan plan;
    plan.sig.in_meta.reserve(inputs.size());
    for (const auto& v: inputs) {
        plan.sig.in_meta.push_back({v.shape(), v.val().dtype(), v.val().device()});
    }
    plan.sig.param_meta.reserve(params.size());
    for (const auto& v: params) {
        plan.sig.param_meta.push_back({v.shape(), v.val().dtype(), v.val().device()});
    }

    auto order = topo_from(output.node.get());
    std::unordered_map<Node*,int> slot_of;
    slot_of.reserve(order.size());

    for (Node* n : order) {
        if (n->op == Op::Leaf) {
            continue;
        }
        Step st;
        st.op = n->op;
        st.out_meta = {n->value.shape().dims, n->value.dtype(), n->value.device()};
        st.out_slot = plan.num_slots++;
        slot_of[n] = st.out_slot;

        st.args.reserve(n->inputs.size());
        for (auto& pin : n->inputs) {
            Node* p = pin.get();
            if (p->op == Op::Leaf) {
                if (is_in(in_ix, p))        st.args.push_back(ArgInput{ in_ix[p] });
                else if (is_in(par_ix, p))  st.args.push_back(ArgParam{ par_ix[p] });
                else                        st.args.push_back(ArgLit{ p->value }); 
            } else {
                st.args.push_back(ArgSlot{ slot_of.at(p) });
            }
        }
        plan.steps.push_back(std::move(st));
    }

    plan.out_slot = slot_of.at(output.node.get());
    
    // --- Generate MLIR source and store it ---
    std::string generated_mlir = emitMLIR(plan);
    std::cout << "\nGenerated MLIR Source:\n" << generated_mlir << std::endl;

    Compiled c;
    c.p = std::make_shared<Compiled::Impl>();
    c.p->plan = std::move(plan);
    c.mlir_source = std::move(generated_mlir); // Store the generated string
    return c;
}

bool Compiled::run(const std::vector<Tensor*>& inputs,
                   const std::vector<Tensor*>& params,
                   Tensor& out) const {
    return p->run(inputs, params, out);
}

const std::string& Compiled::getMLIRSource() const {
    return mlir_source;
}


} // namespace ag::jit
