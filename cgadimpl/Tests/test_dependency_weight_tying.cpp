#include <iostream>
#include <iomanip>
#include "ad/ag_all.hpp"

using namespace ag;

int main() {
    std::cout << "========================================" << std::endl;
    std::cout << "Test: Weight Tying / Parameter Sharing" << std::endl;
    std::cout << "========================================" << std::endl;
    std::cout << "\nPattern:" << std::endl;
    std::cout << "   W (shared weight)" << std::endl;
    std::cout << "   | | | |" << std::endl;
    std::cout << "   o1 o2 o3 o4" << std::endl;
    std::cout << "\nSame parameter W is used in 4 different places." << std::endl;
    std::cout << "Common in RNNs (time steps) and Transformers" << std::endl;
    std::cout << "(encoder-decoder weight sharing)." << std::endl;
    std::cout << "========================================\n" << std::endl;

    const int NUM_USES = 4;
    
    // Single shared weight
    Tensor W = Tensor::randn(Shape{{8, 8}}, TensorOptions().with_req_grad(true));
    auto w = make_tensor(W, "W_shared");
    
    std::cout << "Creating " << NUM_USES << " different uses of the same weight W..." << std::endl;
    
    // Create different inputs
    std::vector<Value> inputs;
    for (int i = 0; i < NUM_USES; i++) {
        Tensor X = Tensor::randn(Shape{{1, 8}}, TensorOptions().with_req_grad(false));
        inputs.push_back(make_tensor(X, ("x" + std::to_string(i)).c_str()));
    }

    
    // Use W in multiple operations (simulating RNN time steps)
    std::vector<Value> outputs;
    for (int i = 0; i < NUM_USES; i++) {
        auto h = relu(matmul(inputs[i], w));
        outputs.push_back(h);
        std::cout << "  âœ“ Use " << (i+1) << ": W applied to input x" << i << std::endl;
    }
    
    // Combine all outputs
    auto combined = outputs[0];
    for (size_t i = 1; i < outputs.size(); i++) {
        combined = combined + outputs[i];
    }
    auto loss = sum(combined);
    
    std::cout << "\nâœ“ Graph built!" << std::endl;
    std::cout << "  Weight W has " << NUM_USES << " children" << std::endl;
    std::cout << "  Dependency counter for W: child_grad_count = " << NUM_USES << std::endl;
    
    std::cout << "\n========================================" << std::endl;
    std::cout << "Running PARALLEL backward pass..." << std::endl;
    std::cout << "========================================" << std::endl;
    
    zero_grad(loss);
    backward(loss, nullptr, true);
    
    std::cout << "âœ… Backward pass completed!\n" << std::endl;
    
    auto grad_w = w.grad();
    std::cout << "Gradient verification:" << std::endl;
    std::cout << "  W gradient computed: " << (grad_w.numel() > 0 ? "âœ“" : "âœ—") << std::endl;
    std::cout << "  Gradient numel: " << grad_w.numel() << std::endl;
    
    std::cout << "\nðŸŽ¯ Key Behavior:" << std::endl;
    std::cout << "   The gradient of W is the SUM of gradients from all 4 uses:" << std::endl;
    std::cout << "   " << std::endl;
    std::cout << "   grad(W) = grad_from_use1 + grad_from_use2 " << std::endl;
    std::cout << "           + grad_from_use3 + grad_from_use4" << std::endl;
    std::cout << "\n   The dependency counter ensures:" << std::endl;
    std::cout << "   1. All " << NUM_USES << " gradient contributions are computed" << std::endl;
    std::cout << "   2. These can happen in parallel (independently)" << std::endl;
    std::cout << "   3. W waits until all " << NUM_USES << " are done (counter = 0)" << std::endl;
    std::cout << "   4. The accumulated gradient is correct!" << std::endl;
    
    std::cout << "\n========================================" << std::endl;
    std::cout << "Real-world applications:" << std::endl;
    std::cout << "========================================" << std::endl;
    std::cout << "  â€¢ RNN: Same weight W^h used across T time steps" << std::endl;
    std::cout << "    â†’ grad(W^h) accumulates T contributions" << std::endl;
    std::cout << "\n  â€¢ Transformer: Encoder & decoder share embeddings" << std::endl;
    std::cout << "    â†’ Embedding gradient comes from both sides" << std::endl;
    std::cout << "\n  â€¢ Siamese Networks: Same CNN weights for both inputs" << std::endl;
    std::cout << "    â†’ Weight gradient = sum from both branches" << std::endl;
    std::cout << "\nâœ… The dependency counter handles all these cases!" << std::endl;
    std::cout << "========================================" << std::endl;
    
    return 0;
}
